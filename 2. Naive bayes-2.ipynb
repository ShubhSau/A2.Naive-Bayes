{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc5df49-2b95-41ec-aaaa-8702a91ef6de",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b8b64-97bf-4b51-a6ab-021bfdb5e15d",
   "metadata": {},
   "source": [
    "Probability that an employee uses the health insurance plan: P(Use Plan) = 0.70\n",
    "Probability that an employee who uses the plan is a smoker:P(Smoker | Use Plan) = 0.40\n",
    "\n",
    "Use the formula for conditional probability:\n",
    "\n",
    "P(Smoker | Use Plan) = P(Smoker Uses Plan) / P(Uses Plan)\n",
    "                     = [P(Uses Plan) * P(Smoker | Use Plan)] / P(Uses Plan)\n",
    "                     = [0.70 * 0.40] / 0.70\n",
    "                     = 0.28 / 0.70\n",
    "                     = 0.4\n",
    "                     \n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.40 or 40."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326dd094-d7ec-49ef-9390-d330a85ce115",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80897a0d-f447-4f93-9b35-cbf0040239eb",
   "metadata": {},
   "source": [
    "Difference between Bernoulli Naive Bayes and Multinomial Naive Bayes are:\n",
    "\n",
    "1. Gaussian Naive Bayes:\n",
    "    - The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.\n",
    "    \n",
    "    - Example: It is often used in problems involving real-valued attributes, such as predicting the price of a house based on various continuous features like size, number of bedrooms, and location.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "    - The Multinomial Na√Øve Bayes classifier is used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which category such as Sports, Politics, education, etc. The classifier uses the frequency of words for the predictors.\n",
    "    \n",
    "    - Example: Text classification problems like spam email detection, sentiment analysis, or document categorization are typical applications of Multinomial Naive Bayes.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721f757-2fc3-469d-8ef9-3bd72fde15bb",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b59d3-f803-445a-b4d1-cf4c8f6495d3",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes variants, can handle missing values in a straightforward manner. When dealing with missing values in a Bernoulli Naive Bayes classification problem, you can typically treat them as if they were absent (0) for the features that are missing. However, the specific approach to handling missing values may depend on the context and the nature of the data. Here are a few common strategies:\n",
    "\n",
    "1. Missing Values as Absent (0): This is the simplest approach. You treat missing values as if the corresponding feature is absent (0). This assumes that the missing values are missing completely at random and that the absence of data is informative. In many cases, this approach works well, especially when missing values are not frequent.\n",
    "\n",
    "2. Imputation: Instead of treating missing values as absent, you can use imputation techniques to estimate or replace the missing values. For Bernoulli Naive Bayes, you might replace missing values with the mode (most common value) of the feature or use more sophisticated imputation methods based on the distribution of the data. Imputation can help retain some information from the missing values, but it may introduce bias if not done carefully.\n",
    "\n",
    "3. Creating a \"Missing\" Category: In some cases, missing values may carry their own information. You can create a new category or level for each feature, specifically for missing values. This way, you don't lose the fact that the data was missing, and the classifier can learn from this information if it is informative for the classification task.\n",
    "\n",
    "4. Ignore Instances with Missing Values: Depending on the severity of missing data, you might choose to exclude instances with missing values from your analysis. This can be a valid strategy if the number of instances with missing data is relatively small, and you have enough data remaining for meaningful analysis.\n",
    "\n",
    "It's important to note that the choice of how to handle missing values should depend on your specific problem, the amount of missing data, and the potential impact on the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a2947-2f89-4604-a665-0df6caa7350c",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd978cf2-ad38-435a-8b60-ee8b087ed6d8",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the basic Naive Bayes algorithm that is designed to handle continuous or real-valued features. It's often used when the features are assumed to follow a Gaussian (normal) distribution.\n",
    "\n",
    "In multi-class classification, the goal is to classify instances into one of several possible classes or categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67594ba2-df8a-4b95-b249-913f9aaeac32",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112d55fd-347e-473e-9598-35bf71920b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and compute metrics\n",
    "accuracy_bernoulli = np.mean(cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy'))\n",
    "precision_bernoulli = np.mean(cross_val_score(bernoulli_nb, X, y, cv=10, scoring='precision'))\n",
    "recall_bernoulli = np.mean(cross_val_score(bernoulli_nb, X, y, cv=10, scoring='recall'))\n",
    "f1_score_bernoulli = np.mean(cross_val_score(bernoulli_nb, X, y, cv=10, scoring='f1'))\n",
    "\n",
    "accuracy_multinomial = np.mean(cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy'))\n",
    "precision_multinomial = np.mean(cross_val_score(multinomial_nb, X, y, cv=10, scoring='precision'))\n",
    "recall_multinomial = np.mean(cross_val_score(multinomial_nb, X, y, cv=10, scoring='recall'))\n",
    "f1_score_multinomial = np.mean(cross_val_score(multinomial_nb, X, y, cv=10, scoring='f1'))\n",
    "\n",
    "accuracy_gaussian = np.mean(cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy'))\n",
    "precision_gaussian = np.mean(cross_val_score(gaussian_nb, X, y, cv=10, scoring='precision'))\n",
    "recall_gaussian = np.mean(cross_val_score(gaussian_nb, X, y, cv=10, scoring='recall'))\n",
    "f1_score_gaussian = np.mean(cross_val_score(gaussian_nb, X, y, cv=10, scoring='f1'))\n",
    "\n",
    "# Print the results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_bernoulli)\n",
    "print(\"Precision:\", precision_bernoulli)\n",
    "print(\"Recall:\", recall_bernoulli)\n",
    "print(\"F1 Score:\", f1_score_bernoulli)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_multinomial)\n",
    "print(\"Precision:\", precision_multinomial)\n",
    "print(\"Recall:\", recall_multinomial)\n",
    "print(\"F1 Score:\", f1_score_multinomial)\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_gaussian)\n",
    "print(\"Precision:\", precision_gaussian)\n",
    "print(\"Recall:\", recall_gaussian)\n",
    "print(\"F1 Score:\", f1_score_gaussian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289744d-3af1-48f0-94fd-41332f705eac",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes performed the best in terms of accuracy, precision, and F1 score for this spam classification task. \n",
    "\n",
    "Limitations and Observations:\n",
    "\n",
    "- Imbalanced Data: The dataset might be imbalanced, which can impact the classifiers' performance. The high recall in Gaussian Naive Bayes may be due to the model predicting spam for many instances to capture as many actual spam emails as possible.\n",
    "\n",
    "- Choice of Features: The choice of features and feature engineering can significantly affect the performance of Naive Bayes classifiers. There might be room for improvement by selecting more relevant features or using techniques like TF-IDF for text-based features.\n",
    "\n",
    "- Hyperparameter Tuning: These results are based on default hyperparameters. Hyperparameter tuning could potentially improve the performance of all three classifiers.\n",
    "\n",
    "- Data Preprocessing: Depending on the quality of data preprocessing (e.g., handling missing values, text cleaning), the performance of the classifiers can vary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d5f03-3bf4-4138-89d7-a6b251653c03",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "- Bernoulli Naive Bayes outperformed the other two variants in terms of accuracy, precision, and F1 score. It achieved an accuracy of approximately 88.4%, indicating its ability to correctly classify a large portion of emails.\n",
    "\n",
    "- Multinomial Naive Bayes had lower overall performance compared to Bernoulli Naive Bayes. While it achieved decent results, it couldn't match the accuracy and precision of the Bernoulli variant.\n",
    "\n",
    "- Gaussian Naive Bayes had the highest recall but lower precision, resulting in a balanced F1 score. It correctly identified a significant portion of spam emails but also produced more false positives compared to the other two variants.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
